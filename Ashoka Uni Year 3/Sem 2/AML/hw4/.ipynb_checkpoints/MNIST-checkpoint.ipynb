{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Dependencies\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model, Sequential\n",
    "from keras.objectives import binary_crossentropy\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.utils import plot_model, np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000\n",
      "12000\n",
      "6000\n",
      "Data Loaded\n"
     ]
    }
   ],
   "source": [
    "#Load Data\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "file_data = x_train.astype('float32')/255.0\n",
    "file_data = file_data.reshape((len(file_data),np.prod(file_data.shape[1:])))\n",
    "split_point = file_data.shape\n",
    "split_point = int(split_point[0]/10)\n",
    "\n",
    "part_1 = file_data[:(split_point*7)]\n",
    "part_2 = file_data[(split_point*7):(split_point*9)]\n",
    "part_3 = file_data[(split_point*9):]\n",
    "\n",
    "y_data = np_utils.to_categorical(y_train)\n",
    "\n",
    "part_1_y = y_data[:(split_point*7)]\n",
    "part_2_y = y_data[(split_point*7):(split_point*9)]\n",
    "part_3_y = y_data[(split_point*9):]\n",
    "\n",
    "print (part_1.shape[0])\n",
    "print (part_2.shape[0])\n",
    "print (part_3.shape[0])\n",
    "\n",
    "print (\"Data Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values set\n"
     ]
    }
   ],
   "source": [
    "#Basic set values\n",
    "\n",
    "epochs = 50\n",
    "hidden_dim = 4\n",
    "intermediate_dim = 500\n",
    "input_dim = 784\n",
    "\n",
    "print (\"Values set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function loaded\n"
     ]
    }
   ],
   "source": [
    "#Loss function\n",
    "def vae_loss(y_true,y_pred):\n",
    "    recon = K.sum(K.binary_crossentropy(y_pred,y_true), axis = 1)\n",
    "    kl = 0.5*K.sum(K.exp(log_sig) + K.square(mu) - 1 - log_sig, axis = 1)\n",
    "    \n",
    "    return recon+kl\n",
    "\n",
    "print (\"Loss function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling function loaded\n"
     ]
    }
   ],
   "source": [
    "#Sampling function\n",
    "\n",
    "def sample(args):\n",
    "    mu, log_sig = args\n",
    "    epsilon = K.random_normal(shape = (intermediate_dim,hidden_dim), mean = 0., stddev = 1)\n",
    "    return mu + K.exp(log_sig/2) * epsilon\n",
    "\n",
    "print (\"Sampling function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder built\n"
     ]
    }
   ],
   "source": [
    "#Encoder\n",
    "\n",
    "inputs = Input(shape = (input_dim,))\n",
    "h_q = Dense(512,activation = 'relu')(inputs)\n",
    "mu = Dense(hidden_dim, activation = 'linear')(h_q)\n",
    "log_sig = Dense(hidden_dim, activation = 'linear')(h_q)\n",
    "z = Lambda(sample)([mu, log_sig])\n",
    "\n",
    "print (\"Encoder built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 403,972\n",
      "Trainable params: 403,972\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Encoder model\n",
    "\n",
    "encoder = Model(inputs,mu)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Built\n"
     ]
    }
   ],
   "source": [
    "#Decoder\n",
    "\n",
    "latent_decoder = Dense(512, activation = 'relu')\n",
    "decoder_output = Dense(input_dim, activation = 'sigmoid')\n",
    "\n",
    "h_p = latent_decoder(z)\n",
    "outputs = decoder_output(h_p)\n",
    "\n",
    "print (\"Decoder Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  2560      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  402192    \n",
      "=================================================================\n",
      "Total params: 404,752\n",
      "Trainable params: 404,752\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Decoder/Generator\n",
    "\n",
    "d_in = Input(shape = (hidden_dim,))\n",
    "d_h = latent_decoder(d_in)\n",
    "d_out = decoder_output(d_h)\n",
    "decoder = Model(d_in,d_out)\n",
    "\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 784)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          401920      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            2052        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4)            2052        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (500, 4)             0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 multiple             2560        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 multiple             402192      dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 810,776\n",
      "Trainable params: 810,776\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Variable Autoencoder\n",
    "\n",
    "vae = Model(inputs,outputs)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\quagzlor\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "42000/42000 [==============================] - 6s 131us/step - loss: 1093.3715 - acc: 0.0052\n",
      "Epoch 2/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 470.6628 - acc: 9.5238e-04\n",
      "Epoch 3/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 452.1610 - acc: 4.2857e-04\n",
      "Epoch 4/50\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 445.1728 - acc: 9.5238e-05\n",
      "Epoch 5/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 442.0539 - acc: 1.1905e-04\n",
      "Epoch 6/50\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 440.3043 - acc: 1.1905e-04\n",
      "Epoch 7/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 439.1950 - acc: 1.6667e-04\n",
      "Epoch 8/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 438.4536 - acc: 9.5238e-05\n",
      "Epoch 9/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 437.8058 - acc: 9.5238e-05\n",
      "Epoch 10/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 437.3649 - acc: 7.1429e-05\n",
      "Epoch 11/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 436.9905 - acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 436.5910 - acc: 2.3810e-05\n",
      "Epoch 13/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 436.3118 - acc: 4.7619e-05: 0s - loss: 437.0687 - acc: 3.0769\n",
      "Epoch 14/50\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 435.9856 - acc: 4.7619e-05\n",
      "Epoch 15/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 435.8152 - acc: 2.3810e-05\n",
      "Epoch 16/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 435.6562 - acc: 7.1429e-05\n",
      "Epoch 17/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 435.6419 - acc: 7.1429e-05\n",
      "Epoch 18/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 435.2293 - acc: 7.1429e-05\n",
      "Epoch 19/50\n",
      "42000/42000 [==============================] - 1s 23us/step - loss: 435.1241 - acc: 9.5238e-05\n",
      "Epoch 20/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 435.1336 - acc: 1.4286e-04\n",
      "Epoch 21/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 434.8931 - acc: 9.5238e-05\n",
      "Epoch 22/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 434.7766 - acc: 2.6190e-04\n",
      "Epoch 23/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 434.6290 - acc: 2.1429e-04\n",
      "Epoch 24/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 434.7128 - acc: 2.6190e-04\n",
      "Epoch 25/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 434.4675 - acc: 3.5714e-04\n",
      "Epoch 26/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 434.3917 - acc: 2.8571e-04\n",
      "Epoch 27/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 434.3259 - acc: 5.2381e-04\n",
      "Epoch 28/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 434.3441 - acc: 2.8571e-04\n",
      "Epoch 29/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 434.1355 - acc: 1.4286e-04\n",
      "Epoch 30/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 434.1027 - acc: 4.0476e-04: 0s - loss: 433.9718 - acc: 4.1096e\n",
      "Epoch 31/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 434.1277 - acc: 4.2857e-04\n",
      "Epoch 32/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 433.9746 - acc: 3.3333e-04\n",
      "Epoch 33/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 434.0124 - acc: 4.7619e-04\n",
      "Epoch 34/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 433.9669 - acc: 3.3333e-04\n",
      "Epoch 35/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 433.9012 - acc: 4.7619e-04: 0s - loss: 434.3215 - acc: 3.174\n",
      "Epoch 36/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 433.9066 - acc: 5.7143e-04\n",
      "Epoch 37/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 433.7937 - acc: 3.8095e-04\n",
      "Epoch 38/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 433.7883 - acc: 5.7143e-04\n",
      "Epoch 39/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 433.6594 - acc: 3.8095e-04\n",
      "Epoch 40/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 433.5449 - acc: 4.5238e-04\n",
      "Epoch 41/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 433.4812 - acc: 4.7619e-04\n",
      "Epoch 42/50\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 433.3922 - acc: 3.0952e-04: 1s - loss: 431.1556 - acc: 4.0000e- - ETA: 0s - loss: 432.3197 - \n",
      "Epoch 43/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 433.3445 - acc: 3.5714e-04\n",
      "Epoch 44/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 433.2699 - acc: 4.0476e-04\n",
      "Epoch 45/50\n",
      "42000/42000 [==============================] - 1s 25us/step - loss: 433.1612 - acc: 5.4762e-04\n",
      "Epoch 46/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 433.0478 - acc: 7.6190e-04\n",
      "Epoch 47/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 432.9390 - acc: 7.8571e-04\n",
      "Epoch 48/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 432.8440 - acc: 0.0015\n",
      "Epoch 49/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 432.7896 - acc: 0.0018\n",
      "Epoch 50/50\n",
      "42000/42000 [==============================] - 1s 24us/step - loss: 432.7670 - acc: 0.0021: 0s - loss: 425.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21e8cc82208>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training\n",
    "\n",
    "vae.compile(optimizer = 'adam', loss = vae_loss, metrics = ['accuracy'])\n",
    "vae.fit(part_1,part_1,batch_size = intermediate_dim, nb_epoch = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1 Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2 Start\n",
    "new_part_2 = encoder.predict(part_2)\n",
    "new_part_3 = encoder.predict(part_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "ff_epochs = 20\n",
    "ff_batch = 100\n",
    "opti = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the Feed-Forward Network\n",
    "ff_n = Sequential()\n",
    "ff_n.add(Dense(32, activation = 'relu', input_shape = (new_part_2.shape[1],)))\n",
    "ff_n.add(Dense(32, activation = 'relu'))\n",
    "ff_n.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "ff_n.compile(loss = 'mse', optimizer = opti, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "12000/12000 [==============================] - 1s 104us/step - loss: 0.0899 - acc: 0.1252\n",
      "Epoch 2/20\n",
      "12000/12000 [==============================] - 1s 50us/step - loss: 0.0896 - acc: 0.1452\n",
      "Epoch 3/20\n",
      "12000/12000 [==============================] - 0s 37us/step - loss: 0.0893 - acc: 0.1498\n",
      "Epoch 4/20\n",
      "12000/12000 [==============================] - 1s 46us/step - loss: 0.0892 - acc: 0.1526\n",
      "Epoch 5/20\n",
      "12000/12000 [==============================] - 0s 41us/step - loss: 0.0890 - acc: 0.1534\n",
      "Epoch 6/20\n",
      "12000/12000 [==============================] - 0s 38us/step - loss: 0.0890 - acc: 0.1608\n",
      "Epoch 7/20\n",
      "12000/12000 [==============================] - 0s 37us/step - loss: 0.0888 - acc: 0.1621\n",
      "Epoch 8/20\n",
      "12000/12000 [==============================] - 0s 40us/step - loss: 0.0887 - acc: 0.1658\n",
      "Epoch 9/20\n",
      "12000/12000 [==============================] - 0s 37us/step - loss: 0.0886 - acc: 0.1686\n",
      "Epoch 10/20\n",
      "12000/12000 [==============================] - 0s 36us/step - loss: 0.0885 - acc: 0.1723\n",
      "Epoch 11/20\n",
      "12000/12000 [==============================] - 0s 38us/step - loss: 0.0884 - acc: 0.1739\n",
      "Epoch 12/20\n",
      "12000/12000 [==============================] - 0s 36us/step - loss: 0.0883 - acc: 0.1737\n",
      "Epoch 13/20\n",
      "12000/12000 [==============================] - 0s 36us/step - loss: 0.0882 - acc: 0.1773\n",
      "Epoch 14/20\n",
      "12000/12000 [==============================] - 0s 35us/step - loss: 0.0882 - acc: 0.1751\n",
      "Epoch 15/20\n",
      "12000/12000 [==============================] - 0s 36us/step - loss: 0.0881 - acc: 0.1796\n",
      "Epoch 16/20\n",
      "12000/12000 [==============================] - 0s 39us/step - loss: 0.0881 - acc: 0.1780\n",
      "Epoch 17/20\n",
      "12000/12000 [==============================] - 0s 34us/step - loss: 0.0881 - acc: 0.1771\n",
      "Epoch 18/20\n",
      "12000/12000 [==============================] - 0s 35us/step - loss: 0.0880 - acc: 0.1772\n",
      "Epoch 19/20\n",
      "12000/12000 [==============================] - 0s 34us/step - loss: 0.0880 - acc: 0.1798\n",
      "Epoch 20/20\n",
      "12000/12000 [==============================] - 0s 35us/step - loss: 0.0880 - acc: 0.1798\n",
      "Accuracy: 17.23%\n"
     ]
    }
   ],
   "source": [
    "#Data Fitting and Evaluation\n",
    "\n",
    "ff_n.fit(new_part_2,part_2_y, epochs = ff_epochs, batch_size = ff_batch)\n",
    "scores = ff_n.evaluate(new_part_3,part_3_y,verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After around 10 epochs the accuracy plateus, but changing the \n",
    "#optimizer has large effect.\n",
    "##Batch size has no impact except on the speed of the epochs.\n",
    "#Question 2 Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper Parameters\n",
    "ff_epochs = 20\n",
    "ff_batch = 10\n",
    "opti = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3 Start\n",
    "\n",
    "#Building the Feed-Forward Network\n",
    "ff_n = Sequential()\n",
    "ff_n.add(Dense(32, activation = 'relu', input_shape = (part_1.shape[1],)))\n",
    "ff_n.add(Dense(32, activation = 'relu'))\n",
    "ff_n.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "ff_n.compile(loss = 'mse', optimizer = opti, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "42000/42000 [==============================] - 16s 382us/step - loss: 0.0151 - acc: 0.8996\n",
      "Epoch 2/20\n",
      "42000/42000 [==============================] - 15s 367us/step - loss: 0.0084 - acc: 0.9445\n",
      "Epoch 3/20\n",
      "42000/42000 [==============================] - 15s 355us/step - loss: 0.0067 - acc: 0.9567\n",
      "Epoch 4/20\n",
      "42000/42000 [==============================] - 15s 364us/step - loss: 0.0059 - acc: 0.9621\n",
      "Epoch 5/20\n",
      "42000/42000 [==============================] - 15s 356us/step - loss: 0.0053 - acc: 0.9667\n",
      "Epoch 6/20\n",
      "42000/42000 [==============================] - 15s 360us/step - loss: 0.0048 - acc: 0.9693\n",
      "Epoch 7/20\n",
      "42000/42000 [==============================] - 15s 353us/step - loss: 0.0044 - acc: 0.9726\n",
      "Epoch 8/20\n",
      "42000/42000 [==============================] - 15s 365us/step - loss: 0.0042 - acc: 0.9739\n",
      "Epoch 9/20\n",
      "42000/42000 [==============================] - 15s 346us/step - loss: 0.0038 - acc: 0.9758\n",
      "Epoch 10/20\n",
      "42000/42000 [==============================] - 15s 351us/step - loss: 0.0037 - acc: 0.9772\n",
      "Epoch 11/20\n",
      "42000/42000 [==============================] - 16s 377us/step - loss: 0.0034 - acc: 0.9785\n",
      "Epoch 12/20\n",
      "42000/42000 [==============================] - 17s 407us/step - loss: 0.0034 - acc: 0.9785\n",
      "Epoch 13/20\n",
      "42000/42000 [==============================] - 18s 417us/step - loss: 0.0032 - acc: 0.9806\n",
      "Epoch 14/20\n",
      "42000/42000 [==============================] - 16s 380us/step - loss: 0.0031 - acc: 0.9806\n",
      "Epoch 15/20\n",
      "42000/42000 [==============================] - 17s 404us/step - loss: 0.0031 - acc: 0.9810\n",
      "Epoch 16/20\n",
      "42000/42000 [==============================] - 15s 360us/step - loss: 0.0030 - acc: 0.9820\n",
      "Epoch 17/20\n",
      "42000/42000 [==============================] - 15s 355us/step - loss: 0.0028 - acc: 0.9827\n",
      "Epoch 18/20\n",
      "42000/42000 [==============================] - 15s 347us/step - loss: 0.0027 - acc: 0.9835\n",
      "Epoch 19/20\n",
      "42000/42000 [==============================] - 15s 361us/step - loss: 0.0026 - acc: 0.9842\n",
      "Epoch 20/20\n",
      "42000/42000 [==============================] - 15s 357us/step - loss: 0.0026 - acc: 0.9841\n",
      "Accuracy: 96.98%\n"
     ]
    }
   ],
   "source": [
    "#Data Fitting and Evaluation\n",
    "\n",
    "ff_n.fit(part_1,part_1_y, epochs = ff_epochs, batch_size = ff_batch)\n",
    "scores = ff_n.evaluate(part_3,part_3_y,verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The largest effect is from the optimizer, as it does not affect\n",
    "#the accuracy as much as the learning rate. SGD, for example, \n",
    "#has a gradual increase, whereas adam starts it off at almost the \n",
    "#highest accuracy. The epochs need to be adjsuted accordingly. \n",
    "#Batch size has no impact except on the speed of the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3 Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \"The overall accuracy of the autoencoder is significantly lower than that of the direct feed forward network.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \"The overall accuracy of the autoencoder is significantly lower than that of the direct feed forward network.\n",
    "That may be due to the structuring of it, or other factors, however.\n",
    "That said, when it comes to epochs in the feed forward network, the autoencoder has a set number of epochs in which\n",
    "it reaches the highest accuracy, whereas the direct feed forward network takes a variable number of epochs.\n",
    "\n",
    "As a result, when time is at a premium, the autoencoder may be a better choice.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
